{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9781526,"sourceType":"datasetVersion","datasetId":5992583},{"sourceId":9928441,"sourceType":"datasetVersion","datasetId":6102607},{"sourceId":9933121,"sourceType":"datasetVersion","datasetId":6106172},{"sourceId":9937085,"sourceType":"datasetVersion","datasetId":6109065}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, sys\n# sys.path.append('../')\n# os.chdir('../')\n\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import optim\nfrom tqdm import tqdm\n\nfrom transformers import BertConfig, BertTokenizer\nfrom nltk.tokenize import word_tokenize","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###\n# common functions\n###\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    \ndef count_param(module, trainable=False):\n    if trainable:\n        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n    else:\n        return sum(p.numel() for p in module.parameters())\n    \ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef metrics_to_string(metric_dict):\n    string_list = []\n    for key, value in metric_dict.items():\n        string_list.append('{}:{:.2f}'.format(key, value))\n    return ' '.join(string_list)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set random seed\nset_seed(26092020) #asli\n# set_seed(42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport re\nfrom collections import defaultdict, namedtuple\n\nMetrics = namedtuple('Metrics', 'tp fp fn prec rec fscore')\n\nclass EvalCounts(object):\n    def __init__(self):\n        self.correct_chunk = 0    # number of correctly identified chunks\n        self.correct_tags = 0     # number of correct chunk tags\n        self.found_correct = 0    # number of chunks in corpus\n        self.found_guessed = 0    # number of identified chunks\n        self.token_counter = 0    # token counter (ignores sentence breaks)\n\n        # counts by type\n        self.t_correct_chunk = defaultdict(int)\n        self.t_found_correct = defaultdict(int)\n        self.t_found_guessed = defaultdict(int)\n\n###\n# Evaluate Function\n###        \ndef parse_tag(t):\n    m = re.match(r'^([^-]*)-(.*)$', t)\n    return m.groups() if m else (t, '')\n\ndef start_of_chunk(prev_tag, tag, prev_type, type_):\n    # check if a chunk started between the previous and current word\n    # arguments: previous and current chunk tags, previous and current types\n    chunk_start = False\n\n    if tag == 'B': chunk_start = True\n    if tag == 'S': chunk_start = True\n\n    if prev_tag == 'E' and tag == 'E': chunk_start = True\n    if prev_tag == 'E' and tag == 'I': chunk_start = True\n    if prev_tag == 'S' and tag == 'E': chunk_start = True\n    if prev_tag == 'S' and tag == 'I': chunk_start = True\n    if prev_tag == 'O' and tag == 'E': chunk_start = True\n    if prev_tag == 'O' and tag == 'I': chunk_start = True\n\n    if tag != 'O' and tag != '.' and prev_type != type_:\n        chunk_start = True\n\n    # these chunks are assumed to have length 1\n    if tag == '[': chunk_start = True\n    if tag == ']': chunk_start = True\n\n    return chunk_start\n\ndef end_of_chunk(prev_tag, tag, prev_type, type_):\n    # check if a chunk ended between the previous and current word\n    # arguments: previous and current chunk tags, previous and current types\n    chunk_end = False\n\n    if prev_tag == 'E': chunk_end = True\n    if prev_tag == 'S': chunk_end = True\n\n    if prev_tag == 'B' and tag == 'B': chunk_end = True\n    if prev_tag == 'B' and tag == 'S': chunk_end = True\n    if prev_tag == 'B' and tag == 'O': chunk_end = True\n    if prev_tag == 'I' and tag == 'B': chunk_end = True\n    if prev_tag == 'I' and tag == 'S': chunk_end = True\n    if prev_tag == 'I' and tag == 'O': chunk_end = True\n\n    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n        chunk_end = True\n\n    # these chunks are assumed to have length 1\n    if prev_tag == ']': chunk_end = True\n    if prev_tag == '[': chunk_end = True\n\n    return chunk_end\n\ndef evaluate_fn(guessed, correct, last_correct, last_correct_type, last_guessed, last_guessed_type, in_correct, counts):\n    guessed, guessed_type = parse_tag(guessed)\n    correct, correct_type = parse_tag(correct)\n\n    end_correct = end_of_chunk(last_correct, correct,\n                               last_correct_type, correct_type)\n    end_guessed = end_of_chunk(last_guessed, guessed,\n                               last_guessed_type, guessed_type)\n    start_correct = start_of_chunk(last_correct, correct,\n                                   last_correct_type, correct_type)\n    start_guessed = start_of_chunk(last_guessed, guessed,\n                                   last_guessed_type, guessed_type)\n\n    if in_correct:\n        if (end_correct and end_guessed and\n            last_guessed_type == last_correct_type):\n            in_correct = False\n            counts.correct_chunk += 1\n            counts.t_correct_chunk[last_correct_type] += 1\n        elif (end_correct != end_guessed or guessed_type != correct_type):\n            in_correct = False\n\n    if start_correct and start_guessed and guessed_type == correct_type:\n        in_correct = True\n\n    if start_correct:\n        counts.found_correct += 1\n        counts.t_found_correct[correct_type] += 1\n    if start_guessed:\n        counts.found_guessed += 1\n        counts.t_found_guessed[guessed_type] += 1\n    if correct == guessed and guessed_type == correct_type:\n        counts.correct_tags += 1\n    counts.token_counter += 1\n\n    last_guessed = guessed\n    last_correct = correct\n    last_guessed_type = guessed_type\n    last_correct_type = correct_type\n    \n    return last_correct, last_correct_type, last_guessed, last_guessed_type, in_correct, counts\n    \ndef evaluate(hyps_list, labels_list):\n    counts = EvalCounts()\n    num_features = None       # number of features per line\n    in_correct = False        # currently processed chunks is correct until now\n    last_correct = 'O'        # previous chunk tag in corpus\n    last_correct_type = ''    # type of previously identified chunk tag\n    last_guessed = 'O'        # previously identified chunk tag\n    last_guessed_type = ''    # type of previous chunk tag in corpus\n\n    for hyps, labels in zip(hyps_list, labels_list):\n        for hyp, label in zip(hyps, labels):\n            step_result = evaluate_fn(hyp, label, last_correct, last_correct_type, last_guessed, last_guessed_type, in_correct, counts)\n            last_correct, last_correct_type, last_guessed, last_guessed_type, in_correct, counts = step_result\n        # Boundary between sentence\n        step_result = evaluate_fn('O', 'O', last_correct, last_correct_type, last_guessed, last_guessed_type, in_correct, counts)\n        last_correct, last_correct_type, last_guessed, last_guessed_type, in_correct, counts = step_result\n        \n    if in_correct:\n        counts.correct_chunk += 1\n        counts.t_correct_chunk[last_correct_type] += 1\n\n    return counts\n\n###\n# Calculate Metrics Function\n###\ndef uniq(iterable):\n    seen = set()\n    return [i for i in iterable if not (i in seen or seen.add(i))]\n\ndef calculate_metrics(correct, guessed, total):\n    tp, fp, fn = correct, guessed-correct, total-correct\n    p = 0 if tp + fp == 0 else 1.*tp / (tp + fp)\n    r = 0 if tp + fn == 0 else 1.*tp / (tp + fn)\n    f = 0 if p + r == 0 else (2 * p * r) / (p + r)\n    return Metrics(tp, fp, fn, p, r, f)\n\ndef eval_metrics(counts):\n    c = counts\n    overall = calculate_metrics(\n        c.correct_chunk, c.found_guessed, c.found_correct\n    )\n    by_type = {}\n    for t in uniq(list(c.t_found_correct.keys()) + list(c.t_found_guessed.keys())):\n        by_type[t] = calculate_metrics(\n            c.t_correct_chunk[t], c.t_found_guessed[t], c.t_found_correct[t]\n        )\n    return overall, by_type\n    return overall\n\n###\n# Main Function\n###\ndef conll_evaluation(hyps_list, labels_list):\n    counts = evaluate(hyps_list, labels_list)\n    overall, by_type = eval_metrics(counts)  # Menggunakan fungsi calculate_metrics dari kode sebelumnya\n\n    c = counts\n    acc = c.correct_tags / c.token_counter\n    pre = overall.prec\n    rec = overall.rec\n    f1 = overall.fscore\n    \n    type_macro_pre = 0.0\n    type_macro_rec = 0.0\n    type_macro_f1 = 0.0\n    for k in by_type.keys():\n        type_macro_pre += by_type[k].prec\n        type_macro_rec += by_type[k].rec\n        type_macro_f1 += by_type[k].fscore\n        \n    type_macro_pre = type_macro_pre / float(len(by_type))\n    type_macro_rec = type_macro_rec / float(len(by_type))\n    type_macro_f1 = type_macro_f1 / float(len(by_type))\n    \n    return (acc, pre, rec, f1, type_macro_pre, type_macro_rec, type_macro_f1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport string\nimport torch\nimport re\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\nfrom tqdm import tqdm\n\n\nclass NerGritDataset(Dataset):\n    # Static constant variable\n    LABEL2INDEX = {'I-PERSON': 0, 'B-ORGANISATION': 1, 'I-ORGANISATION': 2, 'B-PLACE': 3, 'I-PLACE': 4, 'O': 5, 'B-PERSON': 6}\n    INDEX2LABEL = {0: 'I-PERSON', 1: 'B-ORGANISATION', 2: 'I-ORGANISATION', 3: 'B-PLACE', 4: 'I-PLACE', 5: 'O', 6: 'B-PERSON'}\n    NUM_LABELS = 7\n    \n    def load_dataset(self, path):\n        # Read file\n        data = open(path,'r').readlines()\n\n        # Prepare buffer\n        dataset = []\n        sentence = []\n        seq_label = []\n        for line in data:\n            if len(line.strip()) > 0:\n                token, label = line[:-1].split('\\t')\n                sentence.append(token)\n                seq_label.append(self.LABEL2INDEX[label])\n            else:\n                dataset.append({\n                    'sentence': sentence,\n                    'seq_label': seq_label\n                })\n                sentence = []\n                seq_label = []\n        return dataset\n    \n    def __init__(self, dataset_path, tokenizer, *args, **kwargs):\n        self.data = self.load_dataset(dataset_path)\n        self.tokenizer = tokenizer\n        \n    def __getitem__(self, index):\n        data = self.data[index]\n        sentence, seq_label = data['sentence'], data['seq_label']\n        \n        # Add CLS token\n        subwords = [self.tokenizer.cls_token_id]\n        subword_to_word_indices = [-1] # For CLS\n        \n        # Add subwords\n        for word_idx, word in enumerate(sentence):\n            subword_list = self.tokenizer.encode(word, add_special_tokens=False)\n            subword_to_word_indices += [word_idx for i in range(len(subword_list))]\n            subwords += subword_list\n            \n        # Add last SEP token\n        subwords += [self.tokenizer.sep_token_id]\n        subword_to_word_indices += [-1]\n        \n        return np.array(subwords), np.array(subword_to_word_indices), np.array(seq_label), data['sentence']\n    \n    def __len__(self):\n        return len(self.data) \n        \nclass NerDataLoader(DataLoader):\n    def __init__(self, max_seq_len=512, *args, **kwargs):\n        super(NerDataLoader, self).__init__(*args, **kwargs)\n        self.collate_fn = self._collate_fn\n        self.max_seq_len = max_seq_len\n        \n    def _collate_fn(self, batch):\n        batch_size = len(batch)\n        max_seq_len = max(map(lambda x: len(x[0]), batch))\n        max_seq_len = min(self.max_seq_len, max_seq_len)\n        max_tgt_len = max(map(lambda x: len(x[2]), batch))\n        \n        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n        mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)\n        subword_to_word_indices_batch = np.full((batch_size, max_seq_len), -1, dtype=np.int64)\n        seq_label_batch = np.full((batch_size, max_tgt_len), -100, dtype=np.int64)\n        \n        seq_list = []\n        for i, (subwords, subword_to_word_indices, seq_label, raw_seq) in enumerate(batch):\n            subwords = subwords[:max_seq_len]\n            subword_to_word_indices = subword_to_word_indices[:max_seq_len]\n\n            subword_batch[i,:len(subwords)] = subwords\n            mask_batch[i,:len(subwords)] = 1\n            subword_to_word_indices_batch[i,:len(subwords)] = subword_to_word_indices\n            seq_label_batch[i,:len(seq_label)] = seq_label\n\n            seq_list.append(raw_seq)\n            \n        return subword_batch, mask_batch, subword_to_word_indices_batch, seq_label_batch, seq_list","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Forward function for word classification\ndef forward_word_classification(model, batch_data, i2w, is_test=False, device='cpu', **kwargs):\n    # Unpack batch data\n    if len(batch_data) == 4:\n        (subword_batch, mask_batch, subword_to_word_indices_batch, label_batch) = batch_data\n        token_type_batch = None\n    elif len(batch_data) == 5:\n        (subword_batch, mask_batch, token_type_batch, subword_to_word_indices_batch, label_batch) = batch_data\n    \n    # Prepare input & label\n    subword_batch = torch.LongTensor(subword_batch)\n    mask_batch = torch.FloatTensor(mask_batch)\n    token_type_batch = torch.LongTensor(token_type_batch) if token_type_batch is not None else None\n    subword_to_word_indices_batch = torch.LongTensor(subword_to_word_indices_batch)\n    label_batch = torch.LongTensor(label_batch)\n\n    if device == \"cuda\":\n        subword_batch = subword_batch.cuda()\n        mask_batch = mask_batch.cuda()\n        token_type_batch = token_type_batch.cuda() if token_type_batch is not None else None\n        subword_to_word_indices_batch = subword_to_word_indices_batch.cuda()\n        label_batch = label_batch.cuda()\n\n    # Forward model\n    outputs = model(subword_batch, subword_to_word_indices_batch, attention_mask=mask_batch, token_type_ids=token_type_batch, labels=label_batch)\n    loss, logits = outputs[:2]\n    \n    # generate prediction & label list\n    list_hyps = []\n    list_labels = []\n    hyps_list = torch.topk(logits, k=1, dim=-1)[1].squeeze(dim=-1)\n    for i in range(len(hyps_list)):\n        hyps, labels = hyps_list[i].tolist(), label_batch[i].tolist()        \n        list_hyp, list_label = [], []\n        for j in range(len(hyps)):\n            if labels[j] == -100:\n                break\n            else:\n                list_hyp.append(i2w[hyps[j]])\n                list_label.append(i2w[labels[j]])\n        list_hyps.append(list_hyp)\n        list_labels.append(list_label)\n        \n    return loss, list_hyps, list_labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import itertools\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n\ndef ner_metrics_fn(list_hyp, list_label):\n    metrics = {}\n    acc, pre, rec, f1, tm_pre, tm_rec, tm_f1 = conll_evaluation(list_hyp, list_label)\n    metrics[\"ACC\"] = acc\n    metrics[\"F1\"] = tm_f1\n    metrics[\"REC\"] = tm_rec\n    metrics[\"PRE\"] = tm_pre\n    return metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import logging\nimport math\nimport os\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\n\nfrom transformers import AlbertPreTrainedModel, BertPreTrainedModel, AlbertModel, BertModel, BertConfig, XLMModel, XLMConfig, XLMRobertaModel, XLMRobertaConfig\nfrom transformers import AutoTokenizer, AutoConfig\n\nclass BertForWordClassification(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        subword_to_word_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`, defaults to :obj:`None`):\n            Labels for computing the token classification loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n\n    Returns:\n        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when ``labels`` is provided) :\n            Classification loss.\n        scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.num_labels)`)\n            Classification scores (before SoftMax).\n        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n        \"\"\"\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n        )\n\n        sequence_output = outputs[0]\n\n        # average the token-level outputs to compute word-level representations\n        max_seq_len = subword_to_word_ids.max() + 1\n        word_latents = []\n        for i in range(max_seq_len):\n            mask = (subword_to_word_ids == i).unsqueeze(dim=-1)\n            word_latents.append((sequence_output * mask).sum(dim=1) / mask.sum())\n        word_batch = torch.stack(word_latents, dim=1)\n\n        sequence_output = self.dropout(word_batch)\n        logits = self.classifier(sequence_output)\n\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), scores, (hidden_states), (attentions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load IndoBert Model","metadata":{}},{"cell_type":"code","source":"# Load Tokenizer and Config\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p2')\nconfig = BertConfig.from_pretrained('indobenchmark/indobert-base-p2')\nconfig.num_labels = NerGritDataset.NUM_LABELS\n\n# Instantiate model\nmodel = BertForWordClassification.from_pretrained('indobenchmark/indobert-base-p2', config=config)\n\n# Simpan model, konfigurasi, dan optimizer setelah pelatihan\nmodel_checkpoint_path = 'model-bert.pt'\ntokenizer_directory = 'tokenizer_directory'\n\n# Buat directory jika belum ada\nos.makedirs(tokenizer_directory, exist_ok=True)\n\n# Simpan model\ntorch.save({\n            'model_state_dict': model.state_dict(),\n            'config': config,\n            }, model_checkpoint_path)\n\n# Simpan tokenizer\ntokenizer.save_pretrained(tokenizer_directory)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"count_param(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare Named Entity Recognition Dataset (NERGrit)","metadata":{}},{"cell_type":"code","source":"train_dataset_path = '/kaggle/input/nergritdata/train_preprocess.txt'\nvalid_dataset_path = '/kaggle/input/nergritdata/valid_preprocess.txt'\ntest_dataset_path = '/kaggle/input/nergritdata/test_preprocess.txt'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Load and process the data\nfile_path = train_dataset_path\nentities = []\n\nwith open(file_path, 'r', encoding='utf-8') as file:\n    for line in file:\n        parts = line.strip().split('\\t')\n        if len(parts) == 2:\n            _, label = parts\n            if label != 'O':  # Only consider labeled entities\n                entities.append(label.split('-')[1])  # Extract entity type (e.g., PERSON, PLACE)\n\n# Count occurrences of each entity type\nentity_counts = Counter(entities)\n\n# Plot bar chart\nplt.figure(figsize=(10, 5))\nplt.bar(entity_counts.keys(), entity_counts.values(), color=['skyblue', 'salmon', 'lightgreen'])\nplt.xlabel('Entity Type')\nplt.ylabel('Count')\nplt.title('Distribution of Entity Types in Dataset')\nplt.show()\n\n# Plot pie chart\nplt.figure(figsize=(7, 7))\nplt.pie(entity_counts.values(), labels=entity_counts.keys(), autopct='%1.1f%%', colors=['skyblue', 'salmon', 'lightgreen'])\nplt.title('Entity Types Proportion in Dataset')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fungsi untuk menghitung perbandingan jumlah sampel di antara dataset\ndef dataset_split_ratio(train_dataset, valid_dataset, test_dataset):\n    # Hitung jumlah sampel di masing-masing dataset\n    train_size = len(train_dataset)\n    valid_size = len(valid_dataset)\n    test_size = len(test_dataset)\n    total_size = train_size + valid_size + test_size\n    \n    # Hitung persentase tiap dataset\n    train_ratio = (train_size / total_size) * 100\n    valid_ratio = (valid_size / total_size) * 100\n    test_ratio = (test_size / total_size) * 100\n\n    # Tampilkan hasil\n    print(f\"Total samples: {total_size}\")\n    print(f\"Train samples: {train_size} ({train_ratio:.2f}%)\")\n    print(f\"Valid samples: {valid_size} ({valid_ratio:.2f}%)\")\n    print(f\"Test samples: {test_size} ({test_ratio:.2f}%)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = NerGritDataset(train_dataset_path, tokenizer, lowercase=True)\nvalid_dataset = NerGritDataset(valid_dataset_path, tokenizer, lowercase=True)\ntest_dataset = NerGritDataset(test_dataset_path, tokenizer, lowercase=True)\n\nbatch_size=8\n# num_workers=16 #asli\nnum_workers=4\nmax_seq_len=512 #asli\n\ntrain_loader = NerDataLoader(dataset=train_dataset, max_seq_len=max_seq_len, batch_size=batch_size, num_workers=num_workers, shuffle=True)  \nvalid_loader = NerDataLoader(dataset=valid_dataset, max_seq_len=max_seq_len, batch_size=batch_size, num_workers=num_workers, shuffle=False)  \ntest_loader = NerDataLoader(dataset=test_dataset, max_seq_len=max_seq_len, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n\ndataset_split_ratio(train_dataset, valid_dataset, test_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"w2i, i2w = NerGritDataset.LABEL2INDEX, NerGritDataset.INDEX2LABEL\nprint(w2i)\nprint(i2w)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch = next(iter(train_loader))\nsubword_batch, mask_batch, subword_to_word_indices_batch, seq_label_batch, seq_list = batch\nseq_label_batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test model on sample sentences","metadata":{}},{"cell_type":"code","source":"def word_subword_tokenize(sentence, tokenizer):\n    # Add CLS token\n    subwords = [tokenizer.cls_token_id]\n    subword_to_word_indices = [-1] # For CLS\n\n    # Add subwords\n    for word_idx, word in enumerate(sentence):\n        subword_list = tokenizer.encode(word, add_special_tokens=False)\n        subword_to_word_indices += [word_idx for i in range(len(subword_list))]\n        subwords += subword_list\n\n    # Add last SEP token\n    subwords += [tokenizer.sep_token_id]\n    subword_to_word_indices += [-1]\n\n    return subwords, subword_to_word_indices","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text = word_tokenize('Bung Tomo adalah pahlawan nasional Republik Indonesia')\nsubwords, subword_to_word_indices = word_subword_tokenize(text, tokenizer)\n\nsubwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\nsubword_to_word_indices = torch.LongTensor(subword_to_word_indices).view(1, -1).to(model.device)\nlogits = model(subwords, subword_to_word_indices)[0]\n\npreds = torch.topk(logits, k=1, dim=-1)[1].squeeze().numpy()\nlabels = [i2w[preds[i]] for i in range(len(preds))]\n\npd.DataFrame({'words': text, 'label': labels})\nprint(len(subwords))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine Tuning & Evaluation","metadata":{}},{"cell_type":"code","source":"lr=5e-6\n\noptimizer = optim.AdamW(model.parameters(), lr=lr)\nmodel = model.cuda()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install xlsxwriter\n\nimport torch\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport pandas as pd\nfrom tqdm import tqdm\nimport numpy as np\n\n# Initialize lists to store losses, accuracy, precision, recall, and F1 score\ntrain_losses = []\nvalid_losses = []\ntrain_accuracies = []\nvalid_accuracies = []\ntrain_metrics = []\nvalid_metrics = []\nconfusion_matrices = []\n\n# Hyperparameters\nn_epochs = 10\n\n# NER label mapping\ni2w = {\n    0: 'I-PER',\n    1: 'B-ORG',\n    2: 'I-ORG',\n    3: 'B-LOC',\n    4: 'I-LOC',\n    5: 'O',\n    6: 'B-PER'\n}\n\n# Membuat nama file dinamis berdasarkan lr dan batch_size\nfile_name = f'training_lr{lr}_bs{batch_size}_augmented.xlsx'.replace('.', '_')\n\nfor epoch in range(n_epochs):\n    model.train()\n    torch.set_grad_enabled(True)\n    \n    total_train_loss = 0\n    list_hyp, list_label = [], []\n\n    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n    for i, batch_data in enumerate(train_pbar):\n        # Forward model\n        loss, batch_hyp, batch_label = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n\n        # Update model\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        tr_loss = loss.item()\n        total_train_loss += tr_loss\n\n        # Collect predictions and labels\n        list_hyp += batch_hyp\n        list_label += batch_label\n\n        train_pbar.set_description(f\"(Epoch {epoch+1}) TRAIN LOSS:{total_train_loss/(i+1):.4f} LR:{get_lr(optimizer):.8f}\")\n\n    # Calculate train metric using your ner_metrics_fn\n    train_metric = ner_metrics_fn(list_hyp, list_label)\n    train_accuracies.append(train_metric['F1'])\n    train_metrics.append(train_metric)\n\n    # Store training loss\n    train_losses.append(total_train_loss / len(train_loader))\n\n    # Evaluate on validation\n    model.eval()\n    torch.set_grad_enabled(False)\n    \n    total_loss = 0\n    list_hyp, list_label = [], []\n\n    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n    for i, batch_data in enumerate(pbar):\n        loss, batch_hyp, batch_label = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n        \n        # Calculate total loss\n        valid_loss = loss.item()\n        total_loss += valid_loss\n\n        # Collect predictions and labels\n        list_hyp += batch_hyp\n        list_label += batch_label\n        pbar.set_description(f\"VALID LOSS:{total_loss/(i+1):.4f}\")\n\n    # Calculate validation metric using your ner_metrics_fn\n    valid_metric = ner_metrics_fn(list_hyp, list_label)\n    valid_accuracies.append(valid_metric['F1'])\n    valid_metrics.append(valid_metric)\n\n    # Store validation loss\n    valid_losses.append(total_loss / len(valid_loader))\n\n    # Flatten predictions and true labels\n    flat_list_hyp = list(itertools.chain(*list_hyp))\n    flat_list_label = list(itertools.chain(*list_label))\n\n    # Calculate confusion matrix for the current epoch\n    cm = confusion_matrix(flat_list_label, flat_list_hyp)\n    confusion_matrices.append(cm)\n\n    # Print metrics for the current epoch\n    print(f\"(Epoch {epoch+1}) TRAIN METRICS: {train_metric}\")\n    print(f\"(Epoch {epoch+1}) VALID METRICS: {valid_metric}\")\n    # print(f\"Confusion Matrix for Epoch {epoch+1}:\\n\", cm)\n\n# Save results to Excel with dynamic file name\nwith pd.ExcelWriter(file_name, engine='xlsxwriter') as writer:\n    # Write accuracy, precision, recall, F1, and loss to Excel\n    df_acc_loss = pd.DataFrame({\n        'Epoch': range(1, n_epochs+1),\n        'Train Accuracy': [m['ACC'] for m in train_metrics],\n        'Validation Accuracy': [m['ACC'] for m in valid_metrics],\n        'Train Precision': [m['PRE'] for m in train_metrics],\n        'Validation Precision': [m['PRE'] for m in valid_metrics],\n        'Train Recall': [m['REC'] for m in train_metrics],\n        'Validation Recall': [m['REC'] for m in valid_metrics],\n        'Train F1': [m['F1'] for m in train_metrics],\n        'Validation F1': [m['F1'] for m in valid_metrics],\n        'Train Loss': train_losses,\n        'Validation Loss': valid_losses\n    })\n    df_acc_loss.to_excel(writer, sheet_name='Metrics', index=False)\n\n    # Save confusion matrices to a separate sheet\n    for i, cm in enumerate(confusion_matrices):\n        df_cm = pd.DataFrame(cm, \n                           index=[i2w[i] for i in range(len(i2w))],\n                           columns=[i2w[i] for i in range(len(i2w))])\n        df_cm.to_excel(writer, sheet_name=f'Confusion_Matrix_Epoch_{i+1}')\n    \n    # Save confusion matrix plot as an image in the Excel file\n    workbook = writer.book\n    worksheet = workbook.add_worksheet('Confusion_Matrix_Plots')\n    \n    # Plot and save the confusion matrix of the last epoch\n    plt.figure(figsize=(12, 9))\n    \n    # Create confusion matrix plot with proper labels\n    last_cm_df = pd.DataFrame(confusion_matrices[-1],\n                            index=[i2w[i] for i in range(len(i2w))],\n                            columns=[i2w[i] for i in range(len(i2w))])\n    \n    # Plot heatmap with proper font sizes and rotation\n    sns.heatmap(last_cm_df, annot=True, fmt=\"d\", cmap=\"Blues\",\n                xticklabels=True, yticklabels=True)\n    \n    plt.xlabel(\"Predicted\", fontsize=12)\n    plt.ylabel(\"True\", fontsize=12)\n    plt.title(f\"Confusion Matrix (Epoch {n_epochs})\", fontsize=14, pad=20)\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45, ha='right')\n    plt.yticks(rotation=0)\n    \n    # Adjust layout to prevent label cutoff\n    plt.tight_layout()\n    \n    # Save plot as an image\n    plt.savefig('confusion_matrix_last_epoch.png', bbox_inches='tight', dpi=300)\n    \n    # Insert the image into the Excel worksheet\n    worksheet.insert_image('A1', 'confusion_matrix_last_epoch.png')\n\n    # Save the accuracy and loss plots to the Excel file as well\n    plt.figure(figsize=(14, 7))\n    plt.subplot(1, 2, 1)\n    plt.plot(range(1, n_epochs+1), [m['F1'] for m in train_metrics], label=\"Training F1-SCORE\")\n    plt.plot(range(1, n_epochs+1), [m['F1'] for m in valid_metrics], label=\"Validation F1-SCORE\")\n    plt.legend(loc=\"lower right\")\n    plt.title(\"Training and Validation F1-Score\")\n\n    plt.subplot(1, 2, 2)\n    plt.plot(range(1, n_epochs+1), train_losses, label=\"Training Loss\")\n    plt.plot(range(1, n_epochs+1), valid_losses, label=\"Validation Loss\")\n    plt.legend(loc=\"upper right\")\n    plt.title(\"Training and Validation Loss\")\n    \n    plt.savefig('accuracy_loss_plot.png')\n    worksheet.insert_image('A20', 'accuracy_loss_plot.png')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nimport pandas as pd\nimport itertools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Model Evaluation on Test Data\nmodel.eval()\ntorch.set_grad_enabled(False)\n\nlist_hyp, list_label = [], []\n\ntest_pbar = tqdm(test_loader, leave=True, total=len(test_loader))\nfor i, batch_data in enumerate(test_pbar):\n    # Forward pass\n    _, batch_hyp, batch_label = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n\n    # Collect predictions and labels\n    list_hyp += batch_hyp\n    list_label += batch_label\n\n# Flatten predictions and true labels\nflat_list_hyp = list(itertools.chain(*list_hyp))\nflat_list_label = list(itertools.chain(*list_label))\n\n# Konversi label string ke indeks numerik\nflat_list_hyp_indices = [list(i2w.keys())[list(i2w.values()).index(label)] for label in flat_list_hyp]\nflat_list_label_indices = [list(i2w.keys())[list(i2w.values()).index(label)] for label in flat_list_label]\n\n# Handle missing labels in true labels (y_true)\nmissing_labels = set(i2w.keys()) - set(flat_list_label_indices)\nfor label in missing_labels:\n    flat_list_label_indices.append(label)\n    flat_list_hyp_indices.append(label)\n\n# Calculate confusion matrix\ncm = confusion_matrix(flat_list_label_indices, flat_list_hyp_indices, labels=list(i2w.keys()))\n\n# Save predictions and true labels to a CSV file\ndf_test_result = pd.DataFrame({'True Labels': flat_list_label, 'Predicted Labels': flat_list_hyp})\ndf_test_result.to_csv('test_predictions.csv', index=False)\n\n# Plot confusion matrix\nplt.figure(figsize=(12, 9))\ndf_cm = pd.DataFrame(cm, \n                     index=[i2w[i] for i in range(len(i2w))], \n                     columns=[i2w[i] for i in range(len(i2w))])\nsns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n\nplt.title('Confusion Matrix on Test Data')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.xticks(rotation=45)\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.savefig('confusion_matrix_test.png')\nplt.show()\n\n# Print classification report\nprint(\"Classification Report:\")\nprint(classification_report(flat_list_label_indices, flat_list_hyp_indices, target_names=[i2w[i] for i in range(len(i2w))]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Evaluate on test\n# model.eval()\n# torch.set_grad_enabled(False)\n\n# total_loss, total_correct, total_labels = 0, 0, 0\n# list_hyp, list_label = [], []\n\n# pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n# for i, batch_data in enumerate(pbar):\n#     _, batch_hyp, _ = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n#     list_hyp += batch_hyp\n\n# # Save prediction\n# df = pd.DataFrame({'label':list_hyp}).reset_index()\n# df.to_csv('pred.txt', index=False)\n\n# print(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Simpan model, konfigurasi, dan optimizer setelah pelatihan\nmodel_checkpoint_path = '/kaggle/working/fine_tuned_model_test1.pt'\ntorch.save({\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'config': config,\n            }, model_checkpoint_path)\n\n# Simpan tokenizer\ntokenizer_directory = '/kaggle/working/tokenizer_directory1'\nos.makedirs(tokenizer_directory, exist_ok=True)\ntokenizer.save_pretrained(tokenizer_directory)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test fine-tuned model with sample sentences","metadata":{}},{"cell_type":"code","source":"text = word_tokenize('Bung Tomo adalah pahlawan nasional Republik Indonesia')\nsubwords, subword_to_word_indices = word_subword_tokenize(text, tokenizer)\n\nsubwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\nsubword_to_word_indices = torch.LongTensor(subword_to_word_indices).view(1, -1).to(model.device)\nlogits = model(subwords, subword_to_word_indices)[0]\n\npreds = torch.topk(logits, k=1, dim=-1)[1].squeeze().cpu().numpy()\nlabels = [i2w[preds[i]] for i in range(len(preds))]\n\npd.DataFrame({'words': text, 'label': labels})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text = word_tokenize('Alex berada di Gelora Bung Karno')\nsubwords, subword_to_word_indices = word_subword_tokenize(text, tokenizer)\n\nsubwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\nsubword_to_word_indices = torch.LongTensor(subword_to_word_indices).view(1, -1).to(model.device)\nlogits = model(subwords, subword_to_word_indices)[0]\n\npreds = torch.topk(logits, k=1, dim=-1)[1].squeeze().cpu().numpy()\nlabels = [i2w[preds[i]] for i in range(len(preds))]\n\npd.DataFrame({'words': text, 'label': labels})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom transformers import BertTokenizer\nfrom nltk.tokenize import word_tokenize\n\n# Fungsi untuk memuat model dan tokenizer\ndef load_model_and_tokenizer(model_path, tokenizer_path):\n    checkpoint = torch.load(model_path)\n    model = BertForWordClassification.from_pretrained('indobenchmark/indobert-base-p2', config=checkpoint['config'])\n    model.load_state_dict(checkpoint['model_state_dict'])\n    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n    return model, tokenizer\n\n# Fungsi untuk memprediksi label\ndef predict_labels(text, model, tokenizer, i2w):\n    # Tokenisasi teks\n    words = word_tokenize(text)\n    subwords, subword_to_word_indices = word_subword_tokenize(words, tokenizer)\n    \n    # Konversi token ke tensor PyTorch\n    subwords_tensor = torch.LongTensor(subwords).view(1, -1)\n    subword_to_word_indices = torch.LongTensor(subword_to_word_indices).view(1, -1)\n    \n    # Lakukan inferensi\n    outputs = model(input_ids=subwords_tensor, subword_to_word_ids=subword_to_word_indices)\n    \n    # Ambil label yang diprediksi\n    logits = outputs[0]\n    preds = torch.argmax(logits, dim=2).squeeze().tolist()\n    labels = [i2w[pred] for pred in preds]\n    \n    # Buat dataframe untuk menampilkan hasil\n    df = pd.DataFrame({'words': words, 'label': labels})\n    \n    return df\n\n# Load model dan tokenizer\nmodel, tokenizer = load_model_and_tokenizer('/kaggle/working/fine_tuned_model_test1.pt', '/kaggle/working/tokenizer_directory1')\n\n# Definisikan kamus label\n# Sesuaikan dengan kamus yang Anda miliki\ni2w = {0: 'I-PERSON', 1: 'B-ORGANISATION', 2: 'I-ORGANISATION', 3: 'B-PLACE', 4: 'I-PLACE', 5: 'O', 6: 'B-PERSON'}\n\n# Meminta input dari pengguna\nuser_input = input(\"Masukkan teks yang ingin Anda prediksi labelnya: \")\n\n# Prediksi label\npredicted_labels_df = predict_labels(user_input, model, tokenizer, i2w)\n\n# Tampilkan hasil\nprint(predicted_labels_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !rm -rf /kaggle/working","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}